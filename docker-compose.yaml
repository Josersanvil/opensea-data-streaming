# Docker compose file for local PySpark 
version: '3.7'

services:
  # === Streaming application
  stream:
    build:
      context: .
      dockerfile: ./docker/socket/Dockerfile
    env_file:
      - .env
    profiles:
      - kafka
    command:
      - --kafka-topic=OpenSeaRawEvents
      - --kafka-broker=kafka:19092
      - --kafka-client-id=opensea-spark-streaming
      - -l=INFO
    depends_on:
      - create-kafka-topic-raw-events
    # === Jupyter Notebook ===
  notebook:
    build:
      context: ./docker
      dockerfile: ./spark/Dockerfile
    volumes:
      - ./workdir:/app
      - ./docker/conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
    user: root
    command:
      - jupyter
      - lab
      - --NotebookApp.token=''
      - --NotebookApp.disable_check_xsrf=True
      - --NotebookApp.password=''
      - --ip=0.0.0.0
      - --allow-root
    environment:
      - JUPYTER_ENABLE_LAB="yes"
      - GRANT_SUDO="yes"
    ports:
      - "8888:8888"
      - "4040:4040"

  # === Apache Spark ===
  spark:
    build:
      context: ./docker
      dockerfile: ./spark/Dockerfile
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    ports:
      - "8080:8080"
      - "8081:8081"
    volumes:
      - ./docker/conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "-c",
          "from urllib.request import urlopen; urlopen('http://localhost:8080').read()"
        ]
      interval: 5s
      timeout: 5s
      retries: 5

  spark-worker:
    image: docker.io/bitnami/spark:3.5
    depends_on:
      - spark
    deploy:
      replicas: 2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark

  # === MinIO ===
  minio:
    hostname: minio
    image: "minio/minio:RELEASE.2023-11-15T20-43-25Z"
    ports:
      - "9001:9001"
      - "9000:9000"
    command: [ "server", "/data", "--console-address", ":9001" ]
    volumes:
      - minio_data:/data
    environment:
      - MINIO_ROOT_USER=minio
      - MINIO_ROOT_PASSWORD=minio123
      - MINIO_ACCESS_KEY=minio
      - MINIO_SECRET_KEY=minio123

  # Create buckets
  minio-bucket:
    image: minio/mc
    depends_on:
      - minio
    entrypoint: [ "/bin/sh", "-c" ]
    command:
      [
        "mc alias set minio http://minio:9000 minio minio123 && mc mb minio/processed-data && mc mb minio/raw-data"
      ]

  # === Apache Cassandra ===
  cassandra:
    image: docker.io/bitnami/cassandra:4.1
    ports:
      # - '7000:7000' # Internode communication
      - '9042:9042'
    volumes:
      - 'cassandra_data:/bitnami'
    profiles:
      - cassandra
      - all
    environment:
      - CASSANDRA_SEEDS=cassandra
      - CASSANDRA_PASSWORD_SEEDER=yes
      - CASSANDRA_PASSWORD=cassandra
    healthcheck:
      test:
        [
          "CMD",
          "cqlsh",
          "-u",
          "cassandra",
          "-p",
          "cassandra",
          "-e",
          "describe cluster"
        ]
      interval: 5s
      timeout: 5s
      retries: 5

  # === Apache Kafka ===
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.2
    hostname: zookeeper
    profiles:
      - kafka
      - all
    # ports:
    #   - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_SERVERS: zookeeper:2888:3888

  kafka:
    image: confluentinc/cp-kafka:7.3.2
    hostname: kafka
    profiles:
      - kafka
      - all
    # ports:
    #   - "9092:9092"
    #   - "29092:29092"
    #   - "9999:9999"
    environment:
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:19092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092,DOCKER://host.docker.internal:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_BROKER_ID: 1
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_JMX_PORT: 9001
      KAFKA_JMX_HOSTNAME: ${DOCKER_HOST_IP:-127.0.0.1}
      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
    volumes:
      - ./docker/monitoring/jmx-exporter:/usr/share/jmx-exporter
    depends_on:
      - zookeeper

  kafka-schema-registry:
    image: confluentinc/cp-schema-registry:7.3.2
    hostname: kafka-schema-registry
    profiles:
      - kafka
      - all
    # ports:
    # - "8091:8081"
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: PLAINTEXT://kafka:19092
      SCHEMA_REGISTRY_HOST_NAME: kafka-schema-registry
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    volumes:
      - ./docker/monitoring/jmx-exporter:/usr/share/jmx-exporter
    depends_on:
      - zookeeper
      - kafka

  kafka-rest-proxy:
    image: confluentinc/cp-kafka-rest:7.3.2
    hostname: kafka-rest-proxy
    profiles:
      - kafka
      - all
    # ports:
    #   - "8082:8082"
    environment:
      KAFKA_REST_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_REST_LISTENERS: http://0.0.0.0:8082/
      KAFKA_REST_SCHEMA_REGISTRY_URL: http://kafka-schema-registry:8081/
      KAFKA_REST_HOST_NAME: kafka-rest-proxy
      KAFKA_REST_BOOTSTRAP_SERVERS: PLAINTEXT://kafka:19092
    depends_on:
      - zookeeper
      - kafka
      - kafka-schema-registry

  kafka-connect:
    image: confluentinc/cp-kafka-connect:7.3.2
    hostname: kafka-connect
    container_name: kafka-connect
    profiles:
      - kafka
      - all
    # ports:
    #   - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "kafka:19092"
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: 'http://kafka-schema-registry:8081'
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://kafka-schema-registry:8081'
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_LOG4J_ROOT_LOGLEVEL: "INFO"
      CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_PLUGIN_PATH: '/usr/share/java,/etc/kafka-connect/jars,/usr/share/confluent-hub-components'
    volumes:
      - ./docker/monitoring/jmx-exporter:/usr/share/jmx-exporter
    depends_on:
      - zookeeper
      - kafka
      - kafka-schema-registry
      - kafka-rest-proxy
    command:
      - bash
      - -c
      - |
        confluent-hub install --no-prompt debezium/debezium-connector-mysql:latest
        confluent-hub install --no-prompt confluentinc/kafka-connect-datagen:0.4.0
        confluent-hub install --no-prompt confluentinc/kafka-connect-s3:latest
        confluent-hub install --no-prompt confluentinc/kafka-connect-jdbc:latest
        /etc/confluent/docker/run

  ksqldb-server:
    image: confluentinc/cp-ksqldb-server:7.3.2
    hostname: ksqldb-server
    profiles:
      - kafka
      - all
    # ports:
    #   - "8088:8088"
    environment:
      KSQL_BOOTSTRAP_SERVERS: PLAINTEXT://kafka:19092
      KSQL_LISTENERS: http://0.0.0.0:8088/
      KSQL_KSQL_SERVICE_ID: ksqldb-server_
    depends_on:
      - zookeeper
      - kafka

  # Create topics
  create-kafka-topic-raw-events:
    image: confluentinc/cp-kafka:7.3.2
    depends_on:
      - kafka
    profiles:
      - kafka
      - all
    entrypoint: [ "/bin/sh", "-c" ]
    command:
      - "kafka-topics --create --topic OpenSeaRawEvents --if-not-exists --partitions 1 --replication-factor 1 --bootstrap-server kafka:19092"

  create-kafka-topic-enriched-events:
    image: confluentinc/cp-kafka:7.3.2
    depends_on:
      - kafka
    profiles:
      - kafka
      - all
    entrypoint: [ "/bin/sh", "-c" ]
    command:
      - "kafka-topics --create --topic OpenSeaEnrichedEvents --if-not-exists --partitions 1 --replication-factor 1 --bootstrap-server kafka:19092"

  # === Conduktor (For Kafka) ===
  conduktor-db:
    image: postgres:14
    hostname: conduktor-db
    profiles:
      - kafka
      - all
    environment:
      POSTGRES_USER: conduktor
      POSTGRES_PASSWORD: conduktor
      POSTGRES_DB: conduktor
    volumes:
      - conduktor_db_data:/var/lib/postgresql/data

  conduktor:
    image: conduktor/conduktor-platform:1.19.1
    ports:
      - "8090:8080"
    volumes:
      - conduktor_data:/var/conduktor
    profiles:
      - kafka
      - all
    environment:
      CDK_ORGANIZATION_NAME: "demo"
      CDK_ADMIN_EMAIL: "admin@admin.io"
      CDK_ADMIN_PASSWORD: "admin"
      CDK_DATABASE_URL: "postgresql://conduktor:conduktor@conduktor-db:5432/conduktor"
      CDK_CLUSTERS_0_ID: "default"
      CDK_CLUSTERS_0_NAME: "My Local Kafka Cluster"
      CDK_CLUSTERS_0_COLOR: "#0013E7"
      CDK_CLUSTERS_0_BOOTSTRAPSERVERS: "PLAINTEXT://kafka:19092"
      CDK_CLUSTERS_0_SCHEMAREGISTRY_URL: "http://kafka-schema-registry:8081"
      CDK_CLUSTERS_0_KAFKACONNECTS_0_URL: "http://kafka-connect:8083"
      CDK_CLUSTERS_0_KAFKACONNECTS_0_NAME: "full stack kafka connect"
    depends_on:
      - conduktor-db

  # === Prometheus and Grafana ===
  prometheus:
    image: prom/prometheus:v2.47.2
    ports:
      - "9090:9090"
    profiles:
      - monitoring
      - all
    volumes:
      - ./docker/monitoring/prometheus:/etc/prometheus

  alertmanager:
    image: prom/alertmanager:v0.26.0
    profiles:
      - monitoring
      - all
    ports:
      - "19093:9093"

  grafana:
    image: grafana/grafana:9.5.19
    profiles:
      - monitoring
      - all
    ports:
      - "3000:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./docker/monitoring/grafana/:/etc/grafana/provisioning/

volumes:
  cassandra_data:
  minio_data:
  conduktor_db_data:
  conduktor_data:
  grafana-data:
