{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reads data from a Kafka Stream using Spark Streaming API\n",
    "and writes aggregated data to another Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install some dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: confluent-kafka in /opt/conda/lib/python3.11/site-packages (2.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install confluent-kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Spark on port 7077\n",
    "spark = SparkSession.builder.master(\"spark://spark:7077\").appName(\"demo-stream\").config(\"spark.cores.max\", 1).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some important variables\n",
    "kafka_server = \"kafka:19092\"\n",
    "kafka_topic_name = \"demo-data-inventory\"\n",
    "\n",
    "topic_schema_key_name = f\"{kafka_topic_name}-key\"\n",
    "topic_schema_value_name = f\"{kafka_topic_name}-value\"\n",
    "schema_registry_address = \"http://kafka-schema-registry:8081\"\n",
    "\n",
    "checkpoints_path = \"s3a://demo-data-agg/checkpoints\"\n",
    "\n",
    "consumer_group_name = \"demo-data-intentory-spark-agg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the schema of the data in our topic from the Kafka Schema Registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key schema:\n",
      "\"long\"\n",
      "Value schema:\n",
      "{\"type\": \"record\", \"connect.name\": \"ksql.inventory\", \"name\": \"inventory\", \"namespace\": \"ksql\", \"fields\": [{\"type\": \"long\", \"name\": \"id\"}, {\"type\": \"long\", \"name\": \"quantity\"}, {\"type\": \"long\", \"name\": \"productid\"}]}\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka.avro.cached_schema_registry_client import (\n",
    "    CachedSchemaRegistryClient,\n",
    ")\n",
    "\n",
    "sr = CachedSchemaRegistryClient(\n",
    "    {\n",
    "        \"url\": schema_registry_address\n",
    "    }\n",
    ")\n",
    "key_schema = str(sr.get_latest_schema(topic_schema_key_name)[1])\n",
    "value_schema = str(sr.get_latest_schema(topic_schema_value_name)[1])\n",
    "print(f\"Key schema:\\n{key_schema}\")\n",
    "print(f\"Value schema:\\n{value_schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data from the 'demo-data-inventory' kafka topic\n",
    "raw_stream_df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_server)\n",
    "    .option(\"subscribe\", kafka_topic_name)\n",
    "    # Start from offset can be \"latest\" or \"earliest\"\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    # Set the consumer group\n",
    "    .option(\"group.id\", consumer_group_name)\n",
    "    .load()\n",
    ")\n",
    "raw_stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confluent Kafka's stores Avro messages in binary format with 5 bytes at the beginning of each message. These 5 bytes are used to store a magic byte and a schema ID. The schema ID is used to retrieve the schema from the schema registry. The schema is then used to decode the rest of the message.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*clQmx1rbPZaqDlAkHF4jvg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- magicByte: binary (nullable = true)\n",
      " |-- valueSchemaId: binary (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stream_df = raw_stream_df.select(\n",
    "    F.expr(\"substring(key, 6, length(key)-5)\").alias(\"key\"),\n",
    "    F.expr(\"substring(value, 6, length(value)-5)\").alias(\"value\"),\n",
    "    F.expr(\"substring(value, 1, 1)\").alias(\"magicByte\"),\n",
    "    F.expr(\"substring(value, 2, 4)\").alias(\"valueSchemaId\"),\n",
    "    \"partition\",\n",
    "    \"offset\",\n",
    "    \"timestamp\",\n",
    "    \"timestampType\",\n",
    ")\n",
    "stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it looks in the console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Structured Streaming query and write to console\n",
    "query = (\n",
    "    stream_df\n",
    "    .writeStream.format(\"console\").option(\"truncate\", False)\n",
    ")\n",
    "streaming_query = query.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the query after checking the output\n",
    "streaming_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's parse the Avro message using the schema we retrieved from the schema registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: long (nullable = true)\n",
      " |-- inventory: struct (nullable = true)\n",
      " |    |-- id: long (nullable = false)\n",
      " |    |-- quantity: long (nullable = false)\n",
      " |    |-- productid: long (nullable = false)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestamp_date: date (nullable = true)\n",
      " |-- timestamp_date_iso: string (nullable = true)\n",
      " |-- timestamp_hour: integer (nullable = true)\n",
      " |-- timestamp_minute: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.avro.functions import from_avro\n",
    "\n",
    "inventory_df = stream_df.select(\n",
    "    from_avro(F.col(\"key\"), key_schema).alias(\"key\"),\n",
    "    from_avro(F.col(\"value\"), value_schema).alias(\"inventory\"),\n",
    "    F.col(\"timestamp\"),\n",
    "    # Date of the timestamp\n",
    "    F.to_date(F.col(\"timestamp\")).alias(\"timestamp_date\"),\n",
    "    # Date as YYYY-MM-DD string\n",
    "    F.date_format(F.col(\"timestamp\"), \"yyyy-MM-dd\").alias(\"timestamp_date_iso\"),\n",
    "    # Hour of the timestamp\n",
    "    F.hour(F.col(\"timestamp\")).alias(\"timestamp_hour\"),\n",
    "    # Minute of the timestamp\n",
    "    F.minute(F.col(\"timestamp\")).alias(\"timestamp_minute\"),\n",
    ")\n",
    "inventory_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the raw inventory data to json in an S3 bucket\n",
    "output_dir = \"s3a://demo-data-agg/raw-inventory/\"\n",
    "query = (\n",
    "    inventory_df.writeStream.format(\"json\")\n",
    "    .option(\"path\", output_dir)\n",
    "    .option(\"checkpointLocation\", f\"{checkpoints_path}/json/raw-inventory\")\n",
    "    .partitionBy(\"timestamp_date_iso\", \"timestamp_hour\", \"timestamp_minute\")\n",
    ")\n",
    "inventory_streaming_query = query.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_streaming_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data in a readable format, let's aggregate it for analysis and send it to another Kafka topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- productid: long (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a windowed aggregation of the inventory data\n",
    "agg_df = (\n",
    "    inventory_df.withWatermark(\"timestamp\", \"1 minutes\")\n",
    "    .groupBy(\n",
    "        F.window(\"timestamp\", \"1 minutes\", \"1 minutes\"),\n",
    "        F.col(\"inventory.productid\").alias(\"productid\"),\n",
    "    )\n",
    "    .agg(F.sum(\"inventory.quantity\").alias(\"quantity\"))\n",
    ")\n",
    "agg_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to Kafka topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      " |-- key: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write the inventory data to a Kafka topic\n",
    "output_topic = \"demo-data-inventory-agg\"\n",
    "# We need to put the data we want in a 'value' column and the key in a 'key' column. We can do this with a struct and then convert it to a JSON string.\n",
    "topic_agg_df = agg_df.select(\n",
    "    F.to_json(\n",
    "        F.struct(\n",
    "            F.col(\"window.start\").alias(\"window_start\"),\n",
    "            F.col(\"window.end\").alias(\"window_end\"),\n",
    "            F.col(\"productid\").alias(\"product_id\"),\n",
    "            F.col(\"quantity\"),\n",
    "        )\n",
    "    )\n",
    "    .cast(\"string\")\n",
    "    .alias(\"value\"),\n",
    "    # Concat the window start and product id to create a unique key\n",
    "    F.concat(\n",
    "        # Parse to ISO format\n",
    "        F.date_format(F.col(\"window.start\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"),\n",
    "        F.lit(\" \"),\n",
    "        F.col(\"productid\").cast(\"string\"),\n",
    "    ).alias(\"key\"),\n",
    ")\n",
    "topic_agg_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Structured Streaming query\n",
    "query = (\n",
    "    topic_agg_df.writeStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:19092\")\n",
    "    .option(\"topic\", output_topic)\n",
    "    .option(\"checkpointLocation\", f\"{checkpoints_path}/kafka/{output_topic}\")\n",
    "    .partitionBy(\"timestamp_date_iso\", \"timestamp_hour\", \"timestamp_minute\")\n",
    ")\n",
    "streaming_query = query.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standalone KafkaConsumer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.5/246.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting avro-python3\n",
      "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: avro-python3\n",
      "  Building wheel for avro-python3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=43991 sha256=14193737e3e14b348fbe44a50946831a0ea26e3484146f3ca6f7295a130b290f\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/31/be/50/1145d9510eb4440893fc0ec676ef9464a05e0f7492a76fbb2c\n",
      "Successfully built avro-python3\n",
      "Installing collected packages: kafka-python, avro-python3\n",
      "Successfully installed avro-python3-1.10.2 kafka-python-2.0.2\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies to consume from the Kafka Topic and parse the Avro schema\n",
    "!pip install kafka-python fastavro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'quantity': 0, 'productid': 0}\n",
      "{'id': 4, 'quantity': 4, 'productid': 4}\n",
      "{'id': 5, 'quantity': 5, 'productid': 5}\n",
      "{'id': 7, 'quantity': 7, 'productid': 7}\n",
      "{'id': 8, 'quantity': 8, 'productid': 8}\n",
      "{'id': 9, 'quantity': 9, 'productid': 9}\n",
      "{'id': 10, 'quantity': 10, 'productid': 10}\n",
      "{'id': 11, 'quantity': 11, 'productid': 11}\n",
      "{'id': 12, 'quantity': 12, 'productid': 12}\n",
      "{'id': 18, 'quantity': 18, 'productid': 18}\n",
      "{'id': 19, 'quantity': 19, 'productid': 19}\n",
      "{'id': 21, 'quantity': 21, 'productid': 21}\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import fastavro\n",
    "import io\n",
    "\n",
    "msg_schema_dict = {  # Avro schema of the data we are consuming\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"inventory\",\n",
    "    \"namespace\": \"ksql\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"id\", \"type\": \"long\"},\n",
    "        {\"name\": \"quantity\", \"type\": \"long\"},\n",
    "        {\"name\": \"productid\", \"type\": \"long\"},\n",
    "    ],\n",
    "    \"connect.name\": \"ksql.inventory\",\n",
    "}\n",
    "msg_avro_schema = fastavro.parse_schema(msg_schema_dict)\n",
    "\n",
    "\n",
    "def deserialize_avro(message: bytes, schema) -> dict:\n",
    "    bytes_reader = io.BytesIO(message)\n",
    "    # First 5 bytes are magic bytes and schema id\n",
    "    bytes_reader.seek(5)\n",
    "    return fastavro.schemaless_reader(bytes_reader, schema)\n",
    "\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    bootstrap_servers=\"kafka:19092\",\n",
    "    auto_offset_reset=\"earliest\",\n",
    "    group_id=\"demo-data-inventory-consumer\",\n",
    "    # Deserialize Avro messages using the schema registry\n",
    "    value_deserializer=lambda m: deserialize_avro(m, msg_avro_schema),\n",
    ")\n",
    "consumer.subscribe([\"demo-data-inventory\"])\n",
    "n = 10  # Number of messages to read\n",
    "for it, message in enumerate(consumer):\n",
    "    print(message.value)\n",
    "    if it > n:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confluent Kafka Python Client\n",
    "We can also use the confluent_kafka library to read from Kafka topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: confluent-kafka in /opt/conda/lib/python3.11/site-packages (2.3.0)\n",
      "Collecting fastavro\n",
      "  Downloading fastavro-1.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Downloading fastavro-1.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fastavro\n",
      "Successfully installed fastavro-1.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install confluent-kafka fastavro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer\n",
    "from confluent_kafka.serialization import SerializationContext, MessageField\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_conf = {\"url\": schema_registry_address}\n",
    "schema_registry_client = SchemaRegistryClient(sr_conf)\n",
    "avro_schema = str(sr.get_latest_schema(topic_schema_value_name)[1])\n",
    "avro_deserializer = AvroDeserializer(schema_registry_client, avro_schema)\n",
    "consumer_conf = {\n",
    "    \"bootstrap.servers\": kafka_server,\n",
    "    \"group.id\": \"demo-data-inventory\",\n",
    "    \"auto.offset.reset\": \"earliest\",\n",
    "}\n",
    "consumer = Consumer(consumer_conf)\n",
    "consumer.subscribe([kafka_topic_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 23, 'quantity': 23, 'productid': 23}\n",
      "{'id': 24, 'quantity': 24, 'productid': 24}\n",
      "{'id': 26, 'quantity': 26, 'productid': 26}\n",
      "{'id': 31, 'quantity': 31, 'productid': 31}\n",
      "{'id': 32, 'quantity': 32, 'productid': 32}\n",
      "{'id': 33, 'quantity': 33, 'productid': 33}\n",
      "{'id': 34, 'quantity': 34, 'productid': 34}\n",
      "{'id': 35, 'quantity': 35, 'productid': 35}\n",
      "{'id': 38, 'quantity': 38, 'productid': 38}\n",
      "{'id': 41, 'quantity': 41, 'productid': 41}\n"
     ]
    }
   ],
   "source": [
    "n_messages = 10 # Number of messages to read\n",
    "while True:\n",
    "    msg = consumer.poll(1.0)\n",
    "    if msg is None:\n",
    "        continue\n",
    "    if msg.error():\n",
    "        print(\"Consumer error: {}\".format(msg.error()))\n",
    "        continue\n",
    "    # Get the message value\n",
    "    value = avro_deserializer(msg.value(), SerializationContext(msg.topic(), MessageField.VALUE))\n",
    "    print(value)\n",
    "    n_messages -= 1\n",
    "    if n_messages <= 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
